import pandas as pd
import numpy as np
import openassetpricing as oap
import statsmodels.formula.api as smf
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
linear_r = LinearRegression()
from sklearn.linear_model import LogisticRegression
logr = LogisticRegression()
from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor()
import xgboost as xgb
xgb_model = xgb.XGBRegressor(objective='reg:squarederror')
from sklearn.svm import SVR
svr = SVR(kernel='rbf')
from sklearn.metrics import mean_squared_error
from sklearn.neural_network import MLPRegressor
#!pip install shap
import shap
#!pip install seaborn
import seaborn as sns
MLP = MLPRegressor(hidden_layer_sizes=(50,), max_iter=1000)

#!pip install lifelines
#!pip install xgboost
openap = oap.OpenAP()


pd.options.display.max_columns = 200


signal_df = openap.dl_signal_doc('pandas')


signal_df


signal_df.head(10).to_csv("signaldoc_head10.csv", index = False)





signal_df['T-Stat'] = pd.to_numeric(signal_df['T-Stat'], errors='coerce')
good_signals_df = signal_df[signal_df['Signal Rep Quality'] == '1_good']

strong_signals_df = good_signals_df[good_signals_df['T-Stat'] > 3]

sampled_strong_signals = strong_signals_df#.sample(n=40, random_state=42)
sampled_strong_acronyms = sampled_strong_signals['Acronym'].tolist()

sampled_strong_signals
filtered_signals_reset = sampled_strong_signals.reset_index(drop=True)
filtered_signals_reset[['Acronym', 'Signal Rep Quality', 'Return', 'T-Stat']]




signals = filtered_signals_reset['Acronym'].tolist() 


port_vw = openap.dl_port('deciles_vw', 'pandas', signals)
ls_df = port_vw[port_vw['port'] == 'LS']


ls_df


ls_matrix = ls_df.pivot(index='date', columns='signalname', values='ret')
ls_matrix


ls_zscore = (ls_matrix - ls_matrix.mean()) / ls_matrix.std()
ls_zscore.dropna()


weights = filtered_signals_reset.set_index('Acronym')['T-Stat']
weights_aligned = weights.reindex(ls_zscore.columns).fillna(0)
weights_normalized = weights_aligned / weights_aligned.sum()
composite_signal = ls_zscore.dot(weights_normalized)
#composite_signal = composite_signal.dropna()


actual_ls_return = ls_matrix.mean(axis=1)
aligned = pd.concat([composite_signal, actual_ls_return.shift(-1)], axis=1).dropna()
aligned.columns = ['predicted', 'actual']

print(aligned.corr())


algos = [linear_r, rf, xgb_model, svr, MLP]
X = ls_zscore.shift(1).interpolate().dropna()

y = ls_matrix.mean(axis=1).loc[X.index] 
print(X.isna().sum())


def summary_stats(series):
    mean = series.mean()
    std = series.std()
    sharpe = mean / std
    t_stat = mean / (std / np.sqrt(len(series)))
    print(f"  Mean Return: {mean:.4f}")
    print(f"  Volatility: {std:.4f}")
    print(f"  Sharpe Ratio: {sharpe:.4f}")
    print(f"  T-Statistic: {t_stat:.4f}")
    print(f"  Start Date: {series.index.min()}")
    print(f"  End Date: {series.index.max()}")
    print("-" * 50)
    return " "

summary_stats(composite_signal)


composite_cumulative_return = composite_signal.cumsum()
actual_cumulative_return = y.cumsum()
composite_cumulative_return = composite_cumulative_return.interpolate()
actual_cumulative_return = actual_cumulative_return.interpolate()


plt.figure(figsize=(10, 6))
plt.plot(composite_cumulative_return.index, composite_cumulative_return, label="Composite Signal", color='blue', linestyle='-', linewidth=2)
plt.plot(actual_cumulative_return.index, actual_cumulative_return, label="Actual", color='green', linestyle=':', linewidth=2)
plt.title("Cumulative Return: Composite Signal vs Actual Values")
plt.xlabel("Date")  # or "Index" if you're using indices
plt.ylabel("Cumulative Return")
plt.legend(loc='best')
plt.savefig("composite_signal.png")

plt.grid(True)
plt.show()

composite_cumulative_return, actual_cumulative_return = composite_cumulative_return.align(actual_cumulative_return, join='inner')
correlation = composite_cumulative_return.corr(actual_cumulative_return)
mse = mean_squared_error(actual_cumulative_return, composite_cumulative_return)

# Calculate summary statistics for Composite Signal
mean_return = composite_cumulative_return.mean()
volatility = composite_cumulative_return.std()
sharpe_ratio = mean_return / volatility

# Print the output summary
print(f"Composite Signal vs Actual Comparison:")
print(f"Correlation: {correlation}")
print(f"Mean Squared Error: {mse}")
print(f"Mean Return: {mean_return}")
print(f"Volatility: {volatility}")
print(f"Sharpe Ratio: {sharpe_ratio}")
print(f"T-Statistic: {composite_cumulative_return.mean() / composite_cumulative_return.std()}")  # Approximate T-statistic for composite signal





plt.figure(figsize=(10, 6))

colors = {
    "LinearRegression": 'blue',
    "RandomForestRegressor": 'orange',
    "XGBRegressor": 'red',
    "SVR": 'black',
    "MLPRegressor": 'yellow'
}
for algo in algos:
    algo.fit(X, y)
    y_test_pred = pd.Series(algo.predict(X), index=X.index)
    print(type(algo).__name__)
    print(f"  Mean Squared Error: {mean_squared_error(y, y_test_pred)}")
    print(f"  Correlation: {y_test_pred.corr(y)}")
    print(summary_stats(y_test_pred))
    plt.plot(y_test_pred.index, y_test_pred.cumsum(), label=f"Predicted - {type(algo).__name__}", color=colors[type(algo).__name__])

plt.plot(y.index, y.cumsum(), label="Actual", color='green', linewidth=2, linestyle=':')
plt.title("Cumulative Actual vs Predicted Values for All Models")
plt.xlabel("Index")
plt.ylabel("Cumulative Value")
plt.legend()  
plt.grid(True) 
plt.show()





# Create a function for summary statistics
def summary_stats(predictions):
    return {
        "Mean Return": predictions.mean(),
        "Volatility": predictions.std(),
        "Sharpe Ratio": predictions.mean() / predictions.std(),
    }

# Define your models
algos = {
    "LinearRegression": LinearRegression(),
    "RandomForestRegressor": RandomForestRegressor(),
    "XGBRegressor": xgb_model,
    "SVR": SVR(),
    "MLPRegressor": MLPRegressor()
}

# Create a list to store model statistics
model_summary = []

# Loop through each model to create a graph, print the stats, and store them
for model_name, model in algos.items():
    # Fit the model
    model.fit(X, y)
    
    # Generate predictions
    y_pred = pd.Series(model.predict(X), index=X.index)
    
    # Calculate statistics
    mse = mean_squared_error(y, y_pred)
    correlation = y_pred.corr(y)
    stats = summary_stats(y_pred)
    t_stat = model.score(X, y)  # T-Statistic of the model's prediction
    
    # Collect stats into a dictionary for each model
    model_stats = {
        "Model": model_name,
        "Mean Squared Error": mse,
        "Correlation": correlation,
        "Mean Return": stats["Mean Return"],
        "Volatility": stats["Volatility"],
        "Sharpe Ratio": stats["Sharpe Ratio"],
        "T-Statistic": t_stat,
        "Start Date": X.index.min(),
        "End Date": X.index.max()
    }
    
    # Append the model stats to the summary list
    model_summary.append(model_stats)
    
# Convert the model summary into a DataFrame for better display
model_summary_df = pd.DataFrame(model_summary)

# Display the summary table
print(model_summary_df)
model_summary_df.to_csv("model_summary.csv", index = False)


import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
import xgboost as xgb

# Example data setup (replace with actual X, y)
# X = pd.DataFrame(... some data ...)
# y = pd.Series(... target variable ...)

# Create a function for summary statistics
def summary_stats(predictions):
    return {
        "Mean Return": predictions.mean(),
        "Volatility": predictions.std(),
        "Sharpe Ratio": predictions.mean() / predictions.std(),
    }

# Define your models
algos = {
    "LinearRegression": LinearRegression(),
    "RandomForestRegressor": RandomForestRegressor(),
    "XGBRegressor": xgb_model,  # Make sure you initialize it correctly
    "SVR": SVR(),
    "MLPRegressor": MLPRegressor()
}

# Loop through each model to create a graph, print the stats, and save the figure
for model_name, model in algos.items():
    # Fit the model
    model.fit(X, y)
    
    # Generate predictions
    y_pred = pd.Series(model.predict(X), index=X.index)
    
    # Plotting
    plt.figure(figsize=(10, 6))
    plt.plot(y_pred.index, y_pred.cumsum(), label=f"Predicted - {model_name}")
    plt.plot(y.index, y.cumsum(), label="Actual", linestyle="--", color='green')
    plt.title(f"Cumulative Actual vs Predicted: {model_name}")
    plt.xlabel("Index")
    plt.ylabel("Cumulative Value")
    plt.legend()
    plt.grid(True)
    
    # Save the figure
    plt.savefig(f"{model_name}_cumulative_return.png")
    plt.close()  # Close the plot to avoid overlap

    # Print model stats
    print(f"Model: {model_name}")
    print(f"  Mean Squared Error: {mean_squared_error(y, y_pred)}")
    print(f"  Correlation: {y_pred.corr(y)}")
    stats = summary_stats(y_pred)
    for stat, value in stats.items():
        print(f"  {stat}: {value}")
    print(f"  T-Statistic: {model.score(X, y)}")  # T-Statistic of the model's prediction
    print(f"  Start Date: {X.index.min()}")
    print(f"  End Date: {X.index.max()}")
    print("-" * 50)






feature_importance = xgb_model.feature_importances_
importance_df2 = pd.DataFrame({
    'Feature': X.columns,
    'Importance': feature_importance
})

importance_df2 = importance_df2.sort_values(by='Importance', ascending=False).head(20)

plt.figure(figsize=(8, 6))
sns.barplot(x='Importance', y='Feature', data=importance_df2, palette='viridis')
plt.title("Top 20 Feature Importance")
plt.savefig("top20feature.png")
plt.show()





#allsignal = openap.dl_all_signals('pandas')


#allsignal.head()


top_20_signals = [
    "XFIN", "TrendFactor", "NetEquityFinance", "TotalAccruals", "grcapx",
    "RDS", "MomOffSeason06YrPlus", "roaq", "NetDebtFinance", "InvGrowth",
    "InvestPPEInv", "hire", "DelDRC", "retConglomerate", "CustomerMomentum",
    "MomSeason16YrPlus", "IndMom", "betaVIX", "IntanEP", "DelLTI"
]
#allsignal_20 = allsignal[["permno", "yyyymm"] + top_20_signals]


#allsignal_20 = allsignal_20.dropna(thresh=int(len(top_20_signals)*0.8))


#allsignal_20.to_csv('allsignal_20.csv.gz', index=False, compression='gzip')


allsignal_20 = pd.read_csv("allsignal_20.csv.gz", compression = 'gzip')


allsignal_20.head(10)


crsp_df = pd.read_csv("crsp_data.csv")


crsp_df.info()


crsp_df.head(10)


crsp_df['date'] = pd.to_datetime(crsp_df['date'])   
crsp_df['yyyymm'] = crsp_df['date'].dt.year * 100 + crsp_df['date'].dt.month  

crsp_df['permno'] = crsp_df['permno'].astype('int64')
crsp_df['yyyymm'] = crsp_df['yyyymm'].astype('int64')

allsignal_20['permno'] = allsignal_20['permno'].astype('int64')
allsignal_20['yyyymm'] = allsignal_20['yyyymm'].astype('int64')

merged_df = pd.merge(crsp_df, allsignal_20, on=["permno", "yyyymm"], how="inner")
merged_df = merged_df.dropna(subset=["ret"])
merged_df


merged_df.head(10).to_csv("merged_df_head10.csv", index = False)





import pandas as pd
from lifelines import CoxPHFitter

merged_df1 = merged_df.sort_values(by=['permno', 'yyyymm'])

merged_df1['future_ret_6m'] = (
    merged_df.groupby('permno')['ret']
    .rolling(window=6, min_periods=1)
    .sum()
    .shift(-6)
    .reset_index(level=0, drop=True)
)

merged_df1['event'] = (merged_df1['future_ret_6m'] <= -0.5).astype(int)

merged_df1['duration'] = 1

if isinstance(allsignal_20, str):
    allsignal_20 = [allsignal_20]


columns_needed = ["duration", "event"] + list(allsignal_20)


survival_df = merged_df1[columns_needed].dropna()

cph = CoxPHFitter()
cph.fit(survival_df, duration_col='duration', event_col='event')

cph.print_summary()



print("Shape:", survival_df.shape)
print("\nData types:")
print(survival_df.dtypes)
print("\nMissing values:")
print(survival_df.isnull().sum())



# Access the summary as a DataFrame and save it to a CSV file
summary_df = cph.summary
summary_df.to_csv("surv_analysis.csv", index=True)

# Print the summary to console (optional)
print(summary_df)















import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import spearmanr

df = merged_df.copy()
df = df.sort_values(['permno', 'date'])
df['ret_1m_fwd'] = df.groupby('permno')['ret'].shift(-1)
df['ret_3m_fwd'] = df.groupby('permno')['ret'].rolling(3, min_periods=1).sum().shift(-3).reset_index(0, drop=True)
df['ret_6m_fwd'] = df.groupby('permno')['ret'].rolling(6, min_periods=1).sum().shift(-6).reset_index(0, drop=True)

signals = [
    'XFIN', 'TrendFactor', 'NetEquityFinance', 'TotalAccruals', 'grcapx',
    'RDS', 'MomOffSeason06YrPlus', 'roaq', 'NetDebtFinance', 'InvGrowth',
    'InvestPPEInv', 'hire', 'DelDRC', 'retConglomerate', 'CustomerMomentum',
    'MomSeason16YrPlus', 'IndMom', 'betaVIX', 'IntanEP', 'DelLTI'
]

results = []

for signal in signals:
    ic_1m_list = []
    ic_3m_list = []
    ic_6m_list = []
    
    grouped = df[['yyyymm', signal, 'ret_1m_fwd', 'ret_3m_fwd', 'ret_6m_fwd']].dropna().groupby('yyyymm')
    
    for yyyymm, group in grouped:
        x = group[signal]
        ic_1m, _ = spearmanr(x, group['ret_1m_fwd'])
        ic_3m, _ = spearmanr(x, group['ret_3m_fwd'])
        ic_6m, _ = spearmanr(x, group['ret_6m_fwd'])
        ic_1m_list.append(ic_1m)
        ic_3m_list.append(ic_3m)
        ic_6m_list.append(ic_6m)
    
    results.append({
        'Signal': signal,
        'IC_1m_mean': np.nanmean(ic_1m_list),
        'IC_3m_mean': np.nanmean(ic_3m_list),
        'IC_6m_mean': np.nanmean(ic_6m_list),
        'IC_1m_std': np.nanstd(ic_1m_list),
        'IC_3m_std': np.nanstd(ic_3m_list),
        'IC_6m_std': np.nanstd(ic_6m_list)
    })

decay_df = pd.DataFrame(results)

decay_df['IC_avg'] = decay_df[['IC_1m_mean', 'IC_3m_mean', 'IC_6m_mean']].mean(axis=1)

top_n = 5
top_signals = decay_df.sort_values('IC_avg', ascending=False).head(top_n)['Signal'].tolist()
decay_df


decay_df.to_csv("decay.csv", index = False)


plt.figure(figsize=(8, 5))

for idx, row in decay_df[decay_df['Signal'].isin(top_signals)].iterrows():
    ic_means = [row['IC_1m_mean'], row['IC_3m_mean'], row['IC_6m_mean']]
    plt.plot(
        [1, 3, 6],
        ic_means,
        marker='o',
        linestyle='-',
        linewidth=2,
        markersize=5,
        label=row['Signal']
    )

plt.xticks([1, 3, 6])
plt.xlabel('Months Ahead', fontsize=12)
plt.ylabel('Average IC (Spearman Rank Correlation)', fontsize=12)
plt.title(f'Signal Decay Over Time (Top {top_n} Signals)', fontsize=14)
plt.grid(axis='y', linestyle='--', alpha=0.3)
plt.legend(
    title='Signals',
    title_fontsize='11',
    fontsize='10',
    loc='best',
    frameon=False
)
plt.tight_layout()
plt.savefig("decay_graph.png")
plt.show()












# --- Signal Engineering Section ---
# Create engineered features based on combinations of top 20 signals

# Interaction terms
allsignal_20_new = allsignal_20
allsignal_20_new['XFIN_NetEquityFinance'] = allsignal_20_new['XFIN'] * allsignal_20_new['NetEquityFinance']
allsignal_20_new['TrendFactor_TotalAccruals'] = allsignal_20_new['TrendFactor'] * allsignal_20_new['TotalAccruals']

# Ratios
allsignal_20_new['NetEquityFinance_to_TotalAccruals'] = allsignal_20_new['NetEquityFinance'] / (allsignal_20_new['TotalAccruals'].replace(0, np.nan))
allsignal_20_new['RDS_to_InvGrowth'] = allsignal_20_new['RDS'] / (allsignal_20_new['InvGrowth'].replace(0, np.nan))

# Moving averages (within each stock)
allsignal_20_new = allsignal_20_new.sort_values(['permno', 'yyyymm'])
for feature in top_20_signals:
    allsignal_20_new[f'{feature}_MA3'] = allsignal_20_new.groupby('permno')[feature].transform(lambda x: x.rolling(window=3, min_periods=1).mean())
    allsignal_20_new[f'{feature}_MA6'] = allsignal_20_new.groupby('permno')[feature].transform(lambda x: x.rolling(window=6, min_periods=1).mean())

# Example: Feature cross-products or higher-order terms if you want
allsignal_20_new['XFIN_squared'] = allsignal_20_new['XFIN'] ** 2

# After doing the feature engineering step
merged_df2 = pd.merge(allsignal_20_new, crsp_df, on=["permno", "yyyymm"], how="inner")

# Drop rows where "ret" is missing (as you did)
merged_df2 = merged_df2.dropna(subset=["ret"])




engineered_features = [col for col in allsignal_20_new.columns if any(keyword in col for keyword in ['_', 'MA3', 'MA6'])]

X_eng = merged_df2[engineered_features].dropna()
y_eng = merged_df2.loc[X_eng.index, 'ret']

rf.fit(X_eng, y_eng)
importance_eng = rf.feature_importances_

print("Top Engineered Features by Importance:")


# Use your clean top 20 signals directly
original_features = top_20_signals

# Build X and y
X_orig = merged_df2[original_features].dropna()
y_orig = merged_df2.loc[X_orig.index, 'ret']

# Fit random forest
rf.fit(X_orig, y_orig)

# Get feature importance
feature_importance_orig = rf.feature_importances_

# Create DataFrame
importance_df_orig = pd.DataFrame({
    'Feature': X_orig.columns,
    'Importance': feature_importance_orig
})

# Sort and print top 10
importance_df_orig = importance_df_orig.sort_values(by='Importance', ascending=False).head(10)

print("Top 10 Original Features by Importance:")
print(importance_df_orig)



importance_eng_df = pd.DataFrame({
    'Feature': X_eng.columns,
    'Importance': importance_eng
})
importance_eng_df = importance_eng_df.sort_values(by='Importance', ascending=False).head(10)

print("Top Engineered Features by Importance:")



print(importance_eng_df.head(10))


importance_eng_df.head(10).to_csv("importance_eng.csv", index = False)


importance_df_orig.to_csv("orig_importance.csv", index = False)











import pandas as pd

na_counts = merged_df.isna().sum()
na_pct    = merged_df.isna().mean() * 100

nan_table = (pd.concat([na_counts, na_pct], axis=1, keys=["# NAs", "% NAs"])
               .sort_values("# NAs", ascending=False))

print("↯  Missing-value overview (sorted by absolute count)\n")
display(nan_table.style.format({"# NAs": "{:,}", "% NAs": "{:.2f}"}))

print("\nAny missing at all?", merged_df.isna().any().any())

print("\nTop 10 columns by % missing:")
display(nan_table.head(10))



import pandas as pd, numpy as np, gc, seaborn as sns, matplotlib.pyplot as plt
from sklearn.pipeline import make_pipeline
from sklearn.impute import SimpleImputer
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.metrics import r2_score

plt.rcParams.update({"figure.figsize": (7, 4), "figure.dpi": 110})

DROP_THR   = 0.80      # drop columns > 80 % NaN
FLAG_THR   = 0.30      # add _nan flag for 30–80 %
SHOW_N     = 40        # try to plot at most this many features
MAX_ROWS   = 10_000    # sample cap per regime
N_ESTIM    = 120       # trees (bump after it runs)
MAX_DEPTH  = 8         # shallow = faster
N_JOBS     = 1         # cores
REGIMES    = {
    "1990s": ("1990", "1999"),
    "2000s": ("2000", "2009"),
    "2010s": ("2010", "2019"),
    "2020s": ("2020", "2024"),
}
# ───────────────────────────────────────────────────────────────────────────

# 1. Datetime guarantee
merged_df["date"] = pd.to_datetime(merged_df["date"])

# 2. NaN overview (optional eyeball)
na_counts, na_pct = merged_df.isna().sum(), merged_df.isna().mean()*100
print("↯  Worst 10 columns by % missing")
print(pd.concat([na_counts, na_pct], axis=1, keys=["# NAs", "% NAs"])
        .sort_values("% NAs", ascending=False).head(10))

# 3. Drop / flag decisions
na_rate   = merged_df.isna().mean()
drop_cols = na_rate.index[na_rate > DROP_THR].tolist()
flag_cols = na_rate.index[(na_rate > FLAG_THR) & (na_rate <= DROP_THR)].tolist()

print("\nDropping >", int(DROP_THR*100), "% NaN :", drop_cols[:5], "…" if len(drop_cols)>5 else "")
print(f"Flagging {int(FLAG_THR*100)}–{int(DROP_THR*100)} % NaN :", flag_cols[:5], "…" if len(flag_cols)>5 else "")

# 4. Build working DataFrame
df = merged_df.drop(columns=drop_cols).copy()
for c in flag_cols:
    df[c + "_nan"] = df[c].isna().astype("uint8")

df.sort_values("date", inplace=True)

meta_cols = ["date", "ret", "yyyymm"]
FEATURES  = [c for c in df.columns if c not in meta_cols]

# 5. ExtraTrees pipeline
pipe = make_pipeline(
    SimpleImputer(strategy="median"),
    ExtraTreesRegressor(
        n_estimators=N_ESTIM,
        max_depth=MAX_DEPTH,
        min_samples_leaf=20,
        random_state=42,
        n_jobs=N_JOBS,
    ),
)

imp_raw, r2_scores = pd.DataFrame(index=FEATURES), {}

# 6. Regime loop
for lab, (start, end) in REGIMES.items():
    slc = df.loc[(df.date >= start) & (df.date <= end)]
    if MAX_ROWS and len(slc) > MAX_ROWS:
        slc = slc.sample(MAX_ROWS, random_state=42)

    X, y = slc[FEATURES], slc["ret"]
    pipe.fit(X, y)
    imp_raw[lab]  = pipe[-1].feature_importances_
    r2_scores[lab] = r2_score(y, pipe.predict(X))

    del X, y, slc; gc.collect()

# 7. Visuals
imp_pct   = imp_raw.div(imp_raw.sum(axis=0), axis=1)
top_feats = imp_raw.mean(axis=1).nlargest(SHOW_N).index
k         = len(top_feats)            # actual number being displayed

ax = imp_pct.loc[top_feats].T.plot.bar(stacked=True, width=0.85)
ax.set_ylabel("Relative importance")
ax.set_title(f"Top-{k} feature importance by decade")    # dynamic title
ax.legend(bbox_to_anchor=(1.02, 1), loc="upper left")
plt.savefig("top22featuredecade.png")
plt.tight_layout(); plt.show()

sns.heatmap(imp_raw.rank(ascending=False).corr("spearman"),
            annot=True, vmin=0, vmax=1, cmap="viridis")
plt.savefig("spearman.png")
plt.title("Spearman correlation of importance rankings"); plt.show()

print("\nIn-sample R² by regime:", r2_scores)






!jupyter nbconvert --to script best_model_10_random_good.ipynb
